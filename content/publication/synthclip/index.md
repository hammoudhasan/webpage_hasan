---
title: "SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?"

# Authors
authors:
- Hasan Abed Al Kader Hammoud
- Hani Itani
- Fabio Pizzati
- Philip Torr
- Adel Bibi
- Bernard Ghanem

author_notes:
- "Equal contribution"
- "Equal contribution"
- ""
- ""
- ""
- ""

date: "2024-02-05"
doi: ""

# Schedule page publish date (NOT publication's date).
publishDate: "2024-02-05T00:00:00Z"

# Publication type.
publication_types: ["1"]

# Publication name and optional abbreviated publication name.
publication: Preprint
publication_short: SynthCLIP

abstract: We present SynthCLIP, a CLIP model trained on entirely synthetic text-image pairs. Leveraging recent text-to-image (TTI) networks and large language models (LLM), we generate synthetic datasets of images and corresponding captions at scale, with no human intervention. In this work, we provide an analysis on CLIP models trained on synthetic data. We provide insights on the data generation strategy, number of samples required, scaling trends, and resulting properties. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and data, are released as open source at this https URL.

summary: 

tags: []

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'https://arxiv.org/abs/2402.01832'

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
# image:
#   caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)'
#   focal_point: ""
#   preview_only: false

# Associated Projects (optional).
# projects: []

# Slides (optional).
slides: ""

---